# ğŸ“Š World Layoffs Data Cleaning Project  

## ğŸ“Œ Overview  
This project focuses on *cleaning and transforming* the "World Layoffs" dataset using SQL. I also used *AI (DeepSeek)* to efficiently validate and correct inconsistencies in the data, improving accuracy and speed.  

### *Description*  
I cleaned a large dataset by removing duplicates, standardizing values, handling missing data, and fixing encoding errors. The final dataset is structured, accurate, and ready for analysis.   

## ğŸ“‚ Files in the Repository  
- raw file.csv  
- clean data.csv  
- projectquery.sql  
- README.md  

## ğŸ” Data Cleaning Process  
1ï¸âƒ£ *Created a staging table* to avoid modifying the original data.  
2ï¸âƒ£ *Identified duplicate records* using ROW_NUMBER() and removed them.  
3ï¸âƒ£ *Standardized industry and location names* by fixing typos and formatting issues.  
4ï¸âƒ£ *Fixed encoding errors* (e.g., "DÃƒÂ¼sseldorf" â†’ "DÃ¼sseldorf").  
5ï¸âƒ£ *Handled missing values* by removing rows where critical fields were empty.  
6ï¸âƒ£ *Converted date fields* to a proper DATE format.  
7ï¸âƒ£ *Final verification* using SQL queries to check for inconsistencies.  

## ğŸš€ Key SQL Techniques Used  
âœ… *CTE (Common Table Expressions) & Window Functions* for duplicate detection.  
âœ… *String functions (TRIM(), LIKE)* for data standardization.  
âœ… *Date formatting (STR_TO_DATE())* for proper data handling.  
âœ… *JOINs and Subqueries* for updating and verifying data.  

## ğŸ† What I Learned  
- *Data cleaning best practices in SQL*  
- *How to automate data validation using AI (DeepSeek)*  
- *Handling encoding errors and inconsistencies in large datasets*  
- *Improved SQL querying and data transformation techniques*  

---
ğŸ”¹ *Feel free to explore the project and provide feedback!* ğŸ”¹
